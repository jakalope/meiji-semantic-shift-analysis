{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_section"
   },
   "source": [
    "# Edo-Meiji Polysemy Analysis - Google Colab Notebook\n",
    "\n",
    "This notebook runs the complete Edo-Meiji semantic shift analysis pipeline.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Setup**: Mount Google Drive and clone the repository\n",
    "2. **Install Dependencies**: Install required Python packages\n",
    "3. **Data Integration**: Automatically uses Meiji data downloaded via [meiji-download-to-drive.ipynb](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/notebooks/meiji-download-to-drive.ipynb) if available\n",
    "4. **Run Pipeline**: Execute the full analysis pipeline:\n",
    "   - Preprocess texts (tokenization, normalization)\n",
    "   - Extract BERT embeddings\n",
    "   - Cluster embeddings and compute polysemy scores\n",
    "   - Compare Edo vs. Meiji eras statistically\n",
    "5. **View Results**: Display visualizations and statistical comparisons\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "**Recommended workflow:**\n",
    "1. First run [meiji-download-to-drive.ipynb](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/notebooks/meiji-download-to-drive.ipynb) to download Meiji period texts from Aozora Bunko\n",
    "2. Then run this notebook - it will automatically detect and use the downloaded data\n",
    "\n",
    "**Fallback:** If you haven't run the download notebook, this notebook will use sample data instead.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Account (for Drive access)\n",
    "- Google Colab runtime (GPU recommended but not required)\n",
    "- (Optional) Meiji data from meiji-download-to-drive.ipynb\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "- With sample data + GPU: ~5-10 minutes (first run may take longer for model downloads)\n",
    "- With sample data + CPU: ~15-20 minutes\n",
    "- With full downloaded data: Significantly longer (depends on data size)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## Step 1: Mount Google Drive\n",
    "\n",
    "This will prompt you to authorize access to your Google Drive.\n",
    "Your work will be saved in `/content/drive/MyDrive/Meiji_Semantic_Shift_Project/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✓ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_setup"
   },
   "source": [
    "## Step 2: Setup Project Directory\n",
    "\n",
    "Create a dedicated folder in your Google Drive for this project.\n",
    "This ensures all your work, intermediate files, and results are persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_project_dir"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define project directory in Google Drive\n",
    "PROJECT_DIR = \"/content/drive/MyDrive/Meiji_Semantic_Shift_Project\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "print(f\"✓ Project directory: {PROJECT_DIR}\")\n",
    "\n",
    "# Change to project directory\n",
    "%cd \"{PROJECT_DIR}\"\n",
    "print(f\"✓ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## Step 3: Clone or Update Repository\n",
    "\n",
    "This cell will:\n",
    "- Clone the repository if it doesn't exist\n",
    "- Pull latest changes if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_or_update"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/jakalope/meiji-semantic-shift-analysis.git\"\n",
    "REPO_NAME = \"meiji-semantic-shift-analysis\"\n",
    "REPO_PATH = os.path.join(PROJECT_DIR, REPO_NAME)\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"Repository already exists at {REPO_PATH}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    %cd \"{REPO_PATH}\"\n",
    "    !git pull\n",
    "    print(\"✓ Repository updated!\")\n",
    "else:\n",
    "    print(f\"Cloning repository to {REPO_PATH}...\")\n",
    "    !git clone {REPO_URL}\n",
    "    %cd \"{REPO_NAME}\"\n",
    "    print(\"✓ Repository cloned!\")\n",
    "\n",
    "# Verify we're in the repo\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mecab_install"
   },
   "source": [
    "### Colab-only: Install MeCab & IPADIC dictionary (run once per session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mecab_system"
   },
   "outputs": [],
   "source": [
    "# Install MeCab and IPADIC dictionary (system packages)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq mecab libmecab-dev mecab-ipadic-utf8\n",
    "\n",
    "print(\"✓ MeCab system packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mecab_python"
   },
   "outputs": [],
   "source": [
    "# Install Python bindings for MeCab\n",
    "!pip install mecab-python3\n",
    "\n",
    "print(\"✓ mecab-python3 installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_mecab"
   },
   "outputs": [],
   "source": [
    "# Verify MeCab installation\n",
    "import MeCab\n",
    "\n",
    "# Colab fix: explicitly use system mecabrc and ipadic-utf8 dictionary path\n",
    "tagger = MeCab.Tagger('-r /etc/mecabrc -d /var/lib/mecab/dic/ipadic-utf8')\n",
    "print(tagger.parse(\"明治時代の本は面白い\").strip())\n",
    "\n",
    "print(\"\\n✓ MeCab is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MeCab note for Colab**: Initialization uses `-r /etc/mecabrc -d /var/lib/mecab/dic/ipadic-utf8` because the python wrapper defaults to a non-existent `/usr/local/etc/mecabrc` path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "## Step 4: Install Dependencies\n",
    "\n",
    "Install all required Python packages from requirements.txt.\n",
    "\n",
    "**Note**: This may take a few minutes on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pip_install"
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All dependencies installed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "python_path"
   },
   "source": [
    "## Step 5: Setup Python Path\n",
    "\n",
    "Add the `src/` directory to Python path so we can import modules.\n",
    "This fixes the import issue mentioned in the project documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_path"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the repository root directory\n",
    "REPO_ROOT = os.getcwd()\n",
    "SRC_PATH = os.path.join(REPO_ROOT, 'src')\n",
    "\n",
    "# Add src directory to Python path\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(f\"✓ Added {SRC_PATH} to Python path\")\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    import utils\n",
    "    import data_preprocess\n",
    "    import embedding_extraction\n",
    "    import polysemy_clustering\n",
    "    import compare_eras\n",
    "    print(\"✓ All modules can be imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please ensure all dependencies are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "## Step 6: Check GPU Availability\n",
    "\n",
    "Check if GPU is available for faster processing.\n",
    "\n",
    "**Tip**: To enable GPU in Colab, go to `Runtime` > `Change runtime type` > Select `GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_device"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"ℹ GPU not available, using CPU\")\n",
    "    print(\"  Note: Processing will be slower. Consider enabling GPU in Runtime settings.\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_dirs"
   },
   "source": [
    "## Step 7: Create Output Directories\n",
    "\n",
    "Create directories for storing intermediate files and results.\n",
    "These will be persisted in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dirs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directories\n",
    "DATA_PROCESSED = os.path.join(REPO_ROOT, 'data', 'processed')\n",
    "DATA_EMBEDDINGS = os.path.join(REPO_ROOT, 'data', 'embeddings')\n",
    "RESULTS_DIR = os.path.join(REPO_ROOT, 'results')\n",
    "LOGS_DIR = os.path.join(REPO_ROOT, 'logs')\n",
    "\n",
    "# Create directories\n",
    "for directory in [DATA_PROCESSED, DATA_EMBEDDINGS, RESULTS_DIR, LOGS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"✓ {directory}\")\n",
    "\n",
    "print(\"\\n✓ All directories created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_section"
   },
   "source": [
    "---\n",
    "\n",
    "# Running the Pipeline\n",
    "\n",
    "Now we'll run the complete analysis pipeline on sample data.\n",
    "Each step is in a separate cell so you can see the progress.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_preprocess"
   },
   "source": [
    "## Pipeline Step 1: Preprocess Texts\n",
    "\n",
    "Tokenize texts, extract word frequencies, and gather contexts for target words.\n",
    "\n",
    "**Input**: \n",
    "- Edo texts from `data/samples/edo/`\n",
    "- Meiji texts from either:\n",
    "  - Google Drive (if you ran meiji-download-to-drive.ipynb): `/content/drive/MyDrive/meiji-semantic-data/meiji/`\n",
    "  - Sample data (fallback): `data/samples/meiji/`\n",
    "\n",
    "**Output**: \n",
    "- Word frequencies (CSV)\n",
    "- Word contexts (JSON)\n",
    "\n",
    "**Time**: ~30 seconds to 1 minute for sample data, longer for full downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_meiji_data"
   },
   "source": [
    "## Setup Meiji Data Path\n",
    "\n",
    "This cell configures the path to Meiji period data.\n",
    "\n",
    "**Two options:**\n",
    "\n",
    "1. **Use downloaded data from Drive** (Recommended): If you've run the [meiji-download-to-drive.ipynb](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/notebooks/meiji-download-to-drive.ipynb) notebook, it saves data to `/content/drive/MyDrive/meiji-semantic-data/meiji/`\n",
    "\n",
    "2. **Use sample data**: Use the built-in sample data in `data/samples/meiji/`\n",
    "\n",
    "The cell below will check if the Drive data exists and use it if available, otherwise fall back to sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure_meiji_path"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path where meiji-download-to-drive.ipynb saves data\n",
    "MEIJI_DRIVE_DIR = Path('/content/drive/MyDrive/meiji-semantic-data/meiji')\n",
    "MEIJI_DRIVE_FILE = MEIJI_DRIVE_DIR / 'meiji_aozora_combined.txt'\n",
    "\n",
    "# Define local meiji data directory in the repo\n",
    "MEIJI_LOCAL_DIR = Path(REPO_ROOT) / 'data' / 'meiji'\n",
    "MEIJI_LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if Drive data exists\n",
    "if MEIJI_DRIVE_FILE.exists():\n",
    "    print(f\"✓ Found Meiji data from meiji-download-to-drive.ipynb\")\n",
    "    print(f\"  Location: {MEIJI_DRIVE_FILE}\")\n",
    "    \n",
    "    # Copy the file to local data directory for processing\n",
    "    import shutil\n",
    "    local_meiji_file = MEIJI_LOCAL_DIR / 'meiji_aozora_combined.txt'\n",
    "    \n",
    "    if not local_meiji_file.exists():\n",
    "        print(f\"  Copying to {local_meiji_file}...\")\n",
    "        shutil.copy(MEIJI_DRIVE_FILE, local_meiji_file)\n",
    "        print(\"  ✓ Copy complete!\")\n",
    "    else:\n",
    "        print(f\"  ✓ Already copied to {local_meiji_file}\")\n",
    "    \n",
    "    # File size check\n",
    "    file_size_mb = local_meiji_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    MEIJI_DATA_DIR = str(MEIJI_LOCAL_DIR)\n",
    "    USE_DOWNLOADED_DATA = True\n",
    "else:\n",
    "    print(\"ℹ Meiji data from Drive not found.\")\n",
    "    print(f\"  Expected location: {MEIJI_DRIVE_FILE}\")\n",
    "    print(\"  Falling back to sample data...\")\n",
    "    \n",
    "    MEIJI_DATA_DIR = os.path.join(REPO_ROOT, 'data', 'samples', 'meiji')\n",
    "    USE_DOWNLOADED_DATA = False\n",
    "    \n",
    "    if os.path.exists(MEIJI_DATA_DIR):\n",
    "        print(f\"  ✓ Using sample data from {MEIJI_DATA_DIR}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Warning: Sample data directory not found at {MEIJI_DATA_DIR}\")\n",
    "\n",
    "print(f\"\\nMeiji data directory: {MEIJI_DATA_DIR}\")\n",
    "print(f\"Using downloaded data: {USE_DOWNLOADED_DATA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_preprocess"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: PREPROCESSING TEXTS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Use configured Meiji data path\n",
    "EDO_DATA_DIR = os.path.join(REPO_ROOT, 'data', 'samples', 'edo')\n",
    "\n",
    "print(f\"Edo data directory:   {EDO_DATA_DIR}\")\n",
    "print(f\"Meiji data directory: {MEIJI_DATA_DIR}\")\n",
    "print()\n",
    "\n",
    "# Run preprocessing\n",
    "!python src/data_preprocess.py \\\n",
    "    --edo-dir \"{EDO_DATA_DIR}\" \\\n",
    "    --meiji-dir \"{MEIJI_DATA_DIR}\" \\\n",
    "    --output data/processed \\\n",
    "    --top-n 20 \\\n",
    "    --min-freq 3 \\\n",
    "    --max-contexts 100\n",
    "\n",
    "print(\"\\n✓ Preprocessing complete!\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "!ls -lh data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_embeddings"
   },
   "source": [
    "## Pipeline Step 2: Extract BERT Embeddings\n",
    "\n",
    "Use Japanese BERT to generate contextual embeddings for each word occurrence.\n",
    "\n",
    "**Model**: cl-tohoku/bert-base-japanese (~400MB)\n",
    "\n",
    "**Note**: First run will download the model, which may take a few minutes.\n",
    "\n",
    "**Time**: \n",
    "- First run with download: ~3-5 minutes\n",
    "- Subsequent runs: ~1-2 minutes (GPU) or ~3-5 minutes (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_embeddings"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: EXTRACTING BERT EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Run embedding extraction\n",
    "!python src/embedding_extraction.py \\\n",
    "    --input data/processed \\\n",
    "    --output data/embeddings \\\n",
    "    --model cl-tohoku/bert-base-japanese \\\n",
    "    --batch-size 16 \\\n",
    "    --device auto\n",
    "\n",
    "print(\"\\n✓ Embedding extraction complete!\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "!ls -lh data/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_clustering"
   },
   "source": [
    "## Pipeline Step 3: Cluster Embeddings & Compute Polysemy\n",
    "\n",
    "Cluster embeddings for each word to estimate number of distinct senses.\n",
    "Calculate polysemy indices based on cluster count and quality.\n",
    "\n",
    "**Method**: K-means clustering with silhouette score evaluation\n",
    "\n",
    "**Time**: ~1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_clustering"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: CLUSTERING & POLYSEMY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Run clustering and polysemy computation\n",
    "!python src/polysemy_clustering.py \\\n",
    "    --input data/embeddings \\\n",
    "    --output results \\\n",
    "    --min-contexts 5\n",
    "\n",
    "print(\"\\n✓ Clustering and polysemy analysis complete!\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "!ls -lh results/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_compare"
   },
   "source": [
    "## Pipeline Step 4: Compare Eras Statistically\n",
    "\n",
    "Compare polysemy scores between Edo and Meiji periods using statistical tests.\n",
    "Generate visualizations showing the comparison.\n",
    "\n",
    "**Tests**: T-test, Mann-Whitney U, Cohen's d effect size\n",
    "\n",
    "**Time**: ~30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_comparison"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: ERA COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Run statistical comparison\n",
    "!python src/compare_eras.py \\\n",
    "    --input results \\\n",
    "    --output results \\\n",
    "    --alpha 0.05\n",
    "\n",
    "print(\"\\n✓ Era comparison complete!\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "!ls -lh results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "---\n",
    "\n",
    "# Viewing Results\n",
    "\n",
    "Now let's examine the results of our analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_stats"
   },
   "source": [
    "## Statistical Comparison Results\n",
    "\n",
    "Load and display the statistical comparison between Edo and Meiji eras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_stats"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load statistical comparison\n",
    "with open('results/statistical_comparison.json', 'r', encoding='utf-8') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STATISTICAL COMPARISON: EDO VS MEIJI\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "print(f\"Sample Sizes:\")\n",
    "print(f\"  Edo period:   {stats['edo_count']} words\")\n",
    "print(f\"  Meiji period: {stats['meiji_count']} words\")\n",
    "print()\n",
    "\n",
    "print(f\"Mean Polysemy Index:\")\n",
    "print(f\"  Edo period:   {stats['edo_mean']:.3f}\")\n",
    "print(f\"  Meiji period: {stats['meiji_mean']:.3f}\")\n",
    "print(f\"  Difference:   {stats['mean_difference']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Statistical Tests:\")\n",
    "print(f\"  T-test p-value:           {stats['ttest_pvalue']:.4f}\")\n",
    "print(f\"  Mann-Whitney p-value:     {stats['mannwhitney_pvalue']:.4f}\")\n",
    "print(f\"  Cohen's d (effect size):  {stats['cohens_d']:.3f}\")\n",
    "print()\n",
    "\n",
    "# Interpret significance\n",
    "alpha = 0.05\n",
    "if stats['ttest_pvalue'] < alpha:\n",
    "    print(f\"✓ Results are statistically significant (p < {alpha})\")\n",
    "else:\n",
    "    print(f\"  Results are not statistically significant (p >= {alpha})\")\n",
    "\n",
    "# Interpret effect size\n",
    "d = abs(stats['cohens_d'])\n",
    "if d < 0.2:\n",
    "    effect = \"negligible\"\n",
    "elif d < 0.5:\n",
    "    effect = \"small\"\n",
    "elif d < 0.8:\n",
    "    effect = \"medium\"\n",
    "else:\n",
    "    effect = \"large\"\n",
    "\n",
    "print(f\"  Effect size: {effect}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_word_level"
   },
   "source": [
    "## Word-Level Comparison\n",
    "\n",
    "View polysemy changes for individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_word_level"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load word-level comparison\n",
    "word_comparison = pd.read_csv('results/word_level_comparison.csv')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WORD-LEVEL POLYSEMY CHANGES\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Sort by absolute change\n",
    "word_comparison['abs_change'] = word_comparison['polysemy_change'].abs()\n",
    "word_comparison_sorted = word_comparison.sort_values('abs_change', ascending=False)\n",
    "\n",
    "print(\"Top words with largest polysemy changes:\")\n",
    "print()\n",
    "display(word_comparison_sorted.head(10)[['word', 'edo_polysemy', 'meiji_polysemy', 'polysemy_change']])\n",
    "\n",
    "print(\"\\nWords with increased polysemy (Meiji > Edo):\")\n",
    "increased = word_comparison[word_comparison['polysemy_change'] > 0].sort_values('polysemy_change', ascending=False)\n",
    "if len(increased) > 0:\n",
    "    display(increased.head(5)[['word', 'edo_polysemy', 'meiji_polysemy', 'polysemy_change']])\n",
    "else:\n",
    "    print(\"  No words with increased polysemy\")\n",
    "\n",
    "print(\"\\nWords with decreased polysemy (Edo > Meiji):\")\n",
    "decreased = word_comparison[word_comparison['polysemy_change'] < 0].sort_values('polysemy_change')\n",
    "if len(decreased) > 0:\n",
    "    display(decreased.head(5)[['word', 'edo_polysemy', 'meiji_polysemy', 'polysemy_change']])\n",
    "else:\n",
    "    print(\"  No words with decreased polysemy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_visualizations"
   },
   "source": [
    "## Visualizations\n",
    "\n",
    "Display the generated plots comparing Edo and Meiji polysemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_plots"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# List of plot files to display\n",
    "plot_files = [\n",
    "    ('results/polysemy_distribution.png', 'Polysemy Distribution Comparison'),\n",
    "    ('results/polysemy_boxplot.png', 'Polysemy Box Plot'),\n",
    "    ('results/cluster_comparison.png', 'Cluster Count Comparison'),\n",
    "    ('results/top_polysemy_changes.png', 'Top Polysemy Changes')\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "for filepath, title in plot_files:\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(\"-\" * len(title))\n",
    "        display(Image(filename=filepath))\n",
    "    else:\n",
    "        print(f\"\\n⚠ {title}: File not found at {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "✓ **Pipeline completed successfully!**\n",
    "\n",
    "### What we did:\n",
    "\n",
    "1. ✓ Configured data sources (using downloaded Meiji data if available, or sample data as fallback)\n",
    "2. ✓ Preprocessed texts from Edo and Meiji periods\n",
    "3. ✓ Extracted contextual BERT embeddings for target words\n",
    "4. ✓ Clustered embeddings to estimate word polysemy\n",
    "5. ✓ Compared polysemy distributions statistically\n",
    "6. ✓ Generated visualizations and reports\n",
    "\n",
    "### Output files (saved in Google Drive):\n",
    "\n",
    "**Processed Data:**\n",
    "- `data/processed/edo_contexts.json` - Extracted Edo word contexts\n",
    "- `data/processed/meiji_contexts.json` - Extracted Meiji word contexts\n",
    "- `data/processed/*_word_frequencies.csv` - Word frequency tables\n",
    "\n",
    "**Embeddings:**\n",
    "- `data/embeddings/edo_embeddings.pkl` - Edo BERT embeddings\n",
    "- `data/embeddings/meiji_embeddings.pkl` - Meiji BERT embeddings\n",
    "\n",
    "**Results:**\n",
    "- `results/statistical_comparison.json` - Statistical test results\n",
    "- `results/word_level_comparison.csv` - Per-word polysemy comparison\n",
    "- `results/*.png` - Visualization plots\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Get more data**: If you used sample data, run [meiji-download-to-drive.ipynb](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/notebooks/meiji-download-to-drive.ipynb) to download full Meiji period texts from Aozora Bunko, then re-run this notebook\n",
    "2. **Explore results**: Use the exploratory analysis notebook for deeper investigation\n",
    "3. **Customize analysis**: Modify parameters in the pipeline cells above\n",
    "4. **Export findings**: Download results from your Google Drive\n",
    "\n",
    "### For Full Analysis:\n",
    "\n",
    "**Meiji data:**\n",
    "1. Run [meiji-download-to-drive.ipynb](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/notebooks/meiji-download-to-drive.ipynb) to download ~32M characters of Meiji texts\n",
    "2. Re-run this notebook - it will automatically detect and use the downloaded data\n",
    "\n",
    "**Edo data:**\n",
    "1. Upload Edo period texts to `data/edo/` in your Google Drive project folder\n",
    "2. Modify the EDO_DATA_DIR variable in the preprocessing cell\n",
    "\n",
    "**Adjust parameters:**\n",
    "- Increase `--top-n` to analyze more words\n",
    "- Adjust `--min-freq` and `--max-contexts` as needed\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **Repository**: [github.com/jakalope/meiji-semantic-shift-analysis](https://github.com/jakalope/meiji-semantic-shift-analysis)\n",
    "- **Meiji Data Download**: [meiji-download-to-drive.ipynb](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/notebooks/meiji-download-to-drive.ipynb)\n",
    "- **BERT Model**: [cl-tohoku/bert-base-japanese](https://huggingface.co/cl-tohoku/bert-base-japanese)\n",
    "- **Text Source**: [Aozora Bunko](https://www.aozora.gr.jp/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Out of Memory Error**\n",
    "- Solution: Reduce `--batch-size` in embedding extraction cell (try 8 or 4)\n",
    "- Or: Use CPU instead of GPU by changing `--device auto` to `--device cpu`\n",
    "\n",
    "**2. MeCab Import Error**\n",
    "- Solution: Run `!pip install mecab-python3` in a cell\n",
    "- Note: MeCab installation issues are rare on Colab\n",
    "\n",
    "**3. Model Download Fails**\n",
    "- Solution: Check internet connection and try running the embedding cell again\n",
    "- The model is cached after first successful download\n",
    "\n",
    "**4. \"Module not found\" Error**\n",
    "- Solution: Re-run the \"Setup Python Path\" cell (Step 5)\n",
    "- Make sure you've run all cells in order\n",
    "\n",
    "**5. Empty Results**\n",
    "- Check: Are there sample text files in `data/samples/edo/` and `data/samples/meiji/`?\n",
    "- Solution: Verify with `!ls -la data/samples/edo/` and `!ls -la data/samples/meiji/`\n",
    "\n",
    "### Need Help?\n",
    "\n",
    "- Check the [GitHub repository](https://github.com/jakalope/meiji-semantic-shift-analysis) for issues\n",
    "- Read the [USAGE.md](https://github.com/jakalope/meiji-semantic-shift-analysis/blob/main/USAGE.md) guide\n",
    "- Open a new issue with your error message and system info\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Edo-Meiji Polysemy Analysis - Google Colab",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}